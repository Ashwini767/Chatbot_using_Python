{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fsE0hL9_zUjO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('chatbot.txt','r',errors='ignore')\n",
        "raw_doc=f.read()\n",
        "raw_doc=raw_doc.lower()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc)\n",
        "word_tokens=nltk.word_tokenize(raw_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KJYnLpwzkiA",
        "outputId": "99202c60-cd2a-4360-c80c-43819423a2bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMK5Vfrq1Fpv",
        "outputId": "51d4915b-2675-4edf-b4a7-2759ab9fc39c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['documentation\\nsearch\\nnltk documentation\\n\\napi reference\\nexample usage\\nmodule index\\nwiki\\nfaq\\nopen issues\\nnltk on github\\ninstallation\\n\\ninstalling nltk\\ninstalling nltk data\\nmore\\n\\nrelease notes\\ncontributing to nltk\\nnltk team\\nnatural language toolkit\\nnltk is a leading platform for building python programs to work with human language data.',\n",
              " 'it provides easy-to-use interfaces to over 50 corpora and lexical resources such as wordnet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength nlp libraries, and an active discussion forum.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYpr6KTU4vZi",
        "outputId": "de44389f-da57-4dfe-842f-ce4c56e7a1a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['documentation', 'search']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "4o_ARzCn426r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GREET_INPUTS = (\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\")\n",
        "GREET_RESPONSES = [\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"I am glad! you are talking to me\"]\n",
        "def greet(sentence):\n",
        "   for word in sentence.split():\n",
        "     if word.lower() in GREET_INPUTS:\n",
        "       return random.choice(GREET_RESPONSES)\n"
      ],
      "metadata": {
        "id": "z-5ALCGS6M0F"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "I5oT4kb49iwA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "    robo1_response=''\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "       robo1_response=robo1_response+\"I am sorry ! I don't understand you\"\n",
        "       return robo1_response\n",
        "    else:\n",
        "      robo1_response = robo1_response+sent_tokens[idx]\n",
        "      return robo1_response\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6nkX4iuP-LLD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print(\"BOT: My name is stark. Let's have a conversation! Also, if you want to exit any time ,just type bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you'):\n",
        "             flag=False\n",
        "             print(\"BOT: you are welcome..\")\n",
        "\n",
        "        else:\n",
        "            if(greet(user_response)!=None):\n",
        "                 print(\"BOT:\"+greet(user_response))\n",
        "            else:\n",
        "               sent_tokens.append(user_response)\n",
        "               word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
        "               final_words=list(set(word_tokens))\n",
        "               print(\"BOT :\",end=\"\")\n",
        "               print(response(user_response))\n",
        "               sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag= False\n",
        "        print(\"BOT : Goodbye! Take care <3\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ8mpcA4Bymf",
        "outputId": "0176ef95-fefd-4e0a-a1bb-befab052c51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: My name is stark. Let's have a conversation! Also, if you want to exit any time ,just type bye!\n",
            "BOT :I am sorry ! I don't understand you\n",
            "BOT:hey\n",
            "BOT :I am sorry ! I don't understand you\n",
            "BOT :documentation\n",
            "search\n",
            "nltk documentation\n",
            "\n",
            "api reference\n",
            "example usage\n",
            "module index\n",
            "wiki\n",
            "faq\n",
            "open issues\n",
            "nltk on github\n",
            "installation\n",
            "\n",
            "installing nltk\n",
            "installing nltk data\n",
            "more\n",
            "\n",
            "release notes\n",
            "contributing to nltk\n",
            "nltk team\n",
            "natural language toolkit\n",
            "nltk is a leading platform for building python programs to work with human language data.\n",
            "BOT :documentation\n",
            "search\n",
            "nltk documentation\n",
            "\n",
            "api reference\n",
            "example usage\n",
            "module index\n",
            "wiki\n",
            "faq\n",
            "open issues\n",
            "nltk on github\n",
            "installation\n",
            "\n",
            "installing nltk\n",
            "installing nltk data\n",
            "more\n",
            "\n",
            "release notes\n",
            "contributing to nltk\n",
            "nltk team\n",
            "natural language toolkit\n",
            "nltk is a leading platform for building python programs to work with human language data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aB6XYAkNEOCI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}